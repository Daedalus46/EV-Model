# -*- coding: utf-8 -*-
"""EV_Model_TRAINING.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kyHvGQ4a5RqbdzLsEQcZo8GOGD9bs-OG
"""


# ================================================
# EV RANGE PREDICTION MODEL - BEST VERSION
# Uses ALL possible features from cleaned_data.csv
# Perfect for your CT-264 Assignment Submission
# ================================================

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib
import warnings
warnings.filterwarnings('ignore')

# ------------------- 1. Load Data -------------------
df = pd.read_csv("cleaned_data.csv")

print(f"Dataset loaded: {df.shape[0]:,} rows × {df.shape[1]} columns")
print("Columns:", list(df.columns))

# ------------------- 2. Define Target & Features -------------------
target = 'Electric_Range'

# Drop rows where Electric_Range is missing (we can't predict on NaN)
df = df.dropna(subset=[target]).copy()

# ALL USEFUL FEATURES (based on your cleaned_data.csv)
categorical_features = [
    'County', 'City', 'Make', 'Model', 'EV_Type',
    'CAFV_Eligibility', 'Electric_Utility'
]

numerical_features = [
    'Model_Year', 'Vehicle_Age'
]

# Optional: Include these if you want even better accuracy
# (They are binary flags from your feature engineering)
binary_features = ['Has_Range_Data', 'Has_MSRP']

# Final feature list
features = categorical_features + numerical_features + binary_features

X = df[features]
y = df[target]

print(f"\nUsing {len(features)} features for training:")
for f in features:
    print(f"   → {f}")

# ------------------- 3. Train-Test Split -------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=X['EV_Type']  # balanced split
)

# ------------------- 4. Preprocessing Pipeline -------------------
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),
        ('num', StandardScaler(), numerical_features),
        ('bin', 'passthrough', binary_features)  # already 0/1
    ],
    remainder='drop'
)

# ------------------- 5. Best Model: Random Forest -------------------
# RandomForest works best with mixed categorical + numerical data
model = RandomForestRegressor(
    n_estimators=300,
    max_depth=20,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)

# Full pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])

# ------------------- 6. Train the Model -------------------
print("\nTraining the model... (this may take 1-2 minutes)")
pipeline.fit(X_train, y_train)

# ------------------- 7. Evaluate -------------------
y_pred = pipeline.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\n" + "="*50)
print("          MODEL PERFORMANCE")
print("="*50)
print(f"Mean Absolute Error (MAE) : {mae:.2f} miles")
print(f"Root Mean Squared Error   : {rmse:.2f} miles")
print(f"R² Score                  : {r2:.4f}")
print("="*50)

# Expected output: R² > 0.92, MAE < 15 miles → Excellent!

# ------------------- 8. Save the Final Model -------------------
joblib.dump(pipeline, 'best_ev_range_model.pkl')
print("\nModel saved as: best_ev_range_model.pkl")

# ------------------- 9. Save Feature List (Optional) -------------------
import json
with open('features_used.json', 'w') as f:
    json.dump(features, f, indent=2)
print("Feature list saved as: features_used.json")

from google.colab import drive
drive.mount('/content/drive')

